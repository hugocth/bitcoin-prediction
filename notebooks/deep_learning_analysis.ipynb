{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d923511-00c9-47ff-841e-64935e9c41e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Bitcoin Prediction using Deep Learning\n",
    "\n",
    "This time, you will build a basic Neural Network model to predict Bitcoin price based on historical Data.\n",
    "This notebook helps you to train a model but you can use it however you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dcca5a38-d51c-4bf6-82b4-a2264949f16a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import helper_functions as hf\n",
    "\n",
    "CURDIR = os.path.dirname(os.getcwd())\n",
    "DATADIR = os.path.join(CURDIR,  \"data\")\n",
    "FIGDIR = os.path.join(CURDIR,  \"figure\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d99ae8fb-f679-4b74-9b2b-54facd666a81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import Data\n",
    "Our Data come from https://blockchain.info/.\n",
    "\n",
    "Here, we load data into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b1dd7725-d08b-477d-a5d0-0fa210b411bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_blockchain = pd.read_csv(os.path.join(DATADIR, \"df_blockchain.csv\"), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e3cf49b8-1239-4d94-a951-ba912f09861c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>market-price</th>\n",
       "      <th>n-transactions-per-block</th>\n",
       "      <th>median-confirmation-time</th>\n",
       "      <th>hash-rate</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>miners-revenue</th>\n",
       "      <th>trade-volume</th>\n",
       "      <th>blocks-size</th>\n",
       "      <th>avg-block-size</th>\n",
       "      <th>...</th>\n",
       "      <th>cost-per-transaction</th>\n",
       "      <th>n-unique-addresses</th>\n",
       "      <th>n-transactions</th>\n",
       "      <th>n-transactions-total</th>\n",
       "      <th>n-transactions-excluding-popular</th>\n",
       "      <th>output-volume</th>\n",
       "      <th>estimated-transaction-volume</th>\n",
       "      <th>estimated-transaction-volume-usd</th>\n",
       "      <th>total-bitcoins</th>\n",
       "      <th>market-cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-09-13</td>\n",
       "      <td>6.88</td>\n",
       "      <td>45.908451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.018816</td>\n",
       "      <td>1.777774e+06</td>\n",
       "      <td>52318.011503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>592.190091</td>\n",
       "      <td>0.019009</td>\n",
       "      <td>...</td>\n",
       "      <td>7.666766</td>\n",
       "      <td>12622.0</td>\n",
       "      <td>6519.0</td>\n",
       "      <td>1497195.0</td>\n",
       "      <td>6519.0</td>\n",
       "      <td>358543.612114</td>\n",
       "      <td>58615.641320</td>\n",
       "      <td>403275.612279</td>\n",
       "      <td>7.257416e+06</td>\n",
       "      <td>5.022014e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-09-14</td>\n",
       "      <td>6.19</td>\n",
       "      <td>42.465753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.263925</td>\n",
       "      <td>1.755425e+06</td>\n",
       "      <td>48306.468911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>594.907367</td>\n",
       "      <td>0.018007</td>\n",
       "      <td>...</td>\n",
       "      <td>7.369408</td>\n",
       "      <td>12408.0</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>1503780.0</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>302619.024544</td>\n",
       "      <td>74521.484625</td>\n",
       "      <td>461287.989830</td>\n",
       "      <td>7.264662e+06</td>\n",
       "      <td>4.540930e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-09-15</td>\n",
       "      <td>5.92</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.914875</td>\n",
       "      <td>1.755425e+06</td>\n",
       "      <td>60431.444952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>597.554226</td>\n",
       "      <td>0.018240</td>\n",
       "      <td>...</td>\n",
       "      <td>7.333913</td>\n",
       "      <td>12988.0</td>\n",
       "      <td>6474.0</td>\n",
       "      <td>1509972.0</td>\n",
       "      <td>6474.0</td>\n",
       "      <td>299226.130646</td>\n",
       "      <td>79422.402932</td>\n",
       "      <td>470180.625359</td>\n",
       "      <td>7.272284e+06</td>\n",
       "      <td>4.322228e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-09-16</td>\n",
       "      <td>5.58</td>\n",
       "      <td>52.176471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.995096</td>\n",
       "      <td>1.755425e+06</td>\n",
       "      <td>34345.021913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>600.362512</td>\n",
       "      <td>0.022136</td>\n",
       "      <td>...</td>\n",
       "      <td>5.466341</td>\n",
       "      <td>12059.0</td>\n",
       "      <td>6209.0</td>\n",
       "      <td>1516381.0</td>\n",
       "      <td>6209.0</td>\n",
       "      <td>674606.861338</td>\n",
       "      <td>82696.853247</td>\n",
       "      <td>461448.441118</td>\n",
       "      <td>7.279040e+06</td>\n",
       "      <td>4.088136e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-09-17</td>\n",
       "      <td>5.18</td>\n",
       "      <td>40.701493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.733308</td>\n",
       "      <td>1.755425e+06</td>\n",
       "      <td>36805.913687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>602.995510</td>\n",
       "      <td>0.017116</td>\n",
       "      <td>...</td>\n",
       "      <td>6.489054</td>\n",
       "      <td>10988.0</td>\n",
       "      <td>5454.0</td>\n",
       "      <td>1522600.0</td>\n",
       "      <td>5454.0</td>\n",
       "      <td>354198.945778</td>\n",
       "      <td>68238.166521</td>\n",
       "      <td>353473.702578</td>\n",
       "      <td>7.285375e+06</td>\n",
       "      <td>3.801833e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  market-price  n-transactions-per-block  \\\n",
       "0  2011-09-13          6.88                 45.908451   \n",
       "1  2011-09-14          6.19                 42.465753   \n",
       "2  2011-09-15          5.92                 41.500000   \n",
       "3  2011-09-16          5.58                 52.176471   \n",
       "4  2011-09-17          5.18                 40.701493   \n",
       "\n",
       "   median-confirmation-time  hash-rate    difficulty  miners-revenue  \\\n",
       "0                       0.0  12.018816  1.777774e+06    52318.011503   \n",
       "1                       0.0  13.263925  1.755425e+06    48306.468911   \n",
       "2                       0.0  12.914875  1.755425e+06    60431.444952   \n",
       "3                       0.0  10.995096  1.755425e+06    34345.021913   \n",
       "4                       0.0  10.733308  1.755425e+06    36805.913687   \n",
       "\n",
       "   trade-volume  blocks-size  avg-block-size  ...  cost-per-transaction  \\\n",
       "0           0.0   592.190091        0.019009  ...              7.666766   \n",
       "1           0.0   594.907367        0.018007  ...              7.369408   \n",
       "2           0.0   597.554226        0.018240  ...              7.333913   \n",
       "3           0.0   600.362512        0.022136  ...              5.466341   \n",
       "4           0.0   602.995510        0.017116  ...              6.489054   \n",
       "\n",
       "   n-unique-addresses  n-transactions  n-transactions-total  \\\n",
       "0             12622.0          6519.0             1497195.0   \n",
       "1             12408.0          6200.0             1503780.0   \n",
       "2             12988.0          6474.0             1509972.0   \n",
       "3             12059.0          6209.0             1516381.0   \n",
       "4             10988.0          5454.0             1522600.0   \n",
       "\n",
       "   n-transactions-excluding-popular  output-volume  \\\n",
       "0                            6519.0  358543.612114   \n",
       "1                            6200.0  302619.024544   \n",
       "2                            6474.0  299226.130646   \n",
       "3                            6209.0  674606.861338   \n",
       "4                            5454.0  354198.945778   \n",
       "\n",
       "   estimated-transaction-volume  estimated-transaction-volume-usd  \\\n",
       "0                  58615.641320                     403275.612279   \n",
       "1                  74521.484625                     461287.989830   \n",
       "2                  79422.402932                     470180.625359   \n",
       "3                  82696.853247                     461448.441118   \n",
       "4                  68238.166521                     353473.702578   \n",
       "\n",
       "   total-bitcoins    market-cap  \n",
       "0    7.257416e+06  5.022014e+07  \n",
       "1    7.264662e+06  4.540930e+07  \n",
       "2    7.272284e+06  4.322228e+07  \n",
       "3    7.279040e+06  4.088136e+07  \n",
       "4    7.285375e+06  3.801833e+07  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blockchain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "08350022-3527-4c51-9261-97f26fde3619",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Explore Dataset\n",
    "\n",
    "We already explored dataset before, but you can draw other plots to analyse data if you want.\n",
    "\n",
    "Idea : you can use pandas_profiling \n",
    "\n",
    "```python\n",
    "from pandas_profiling import ProfileReport\n",
    "ProfileReport(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cefeefeb-e058-4dfd-acca-a9e49fa13181",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3667a62d-4049-4cf9-af24-5eddb67e7c09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get columns (You can add more columns to analyse results)\n",
    "columns = [\"market-price\"]\n",
    "dataset = df_blockchain[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8ec0db25-2468-4d50-ba88-a0bf60dad26b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data scaling\n",
    "\n",
    "Here we scale the price between 0 and 1, this will help the optimization algorithm converge faster.\n",
    "\n",
    "See the following figure (source : Andrew Ng https://www.andrewng.org ) :\n",
    "\n",
    "![alt text](../data/feature-scaling.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4df7ff9f-5f14-4da4-9e3d-b3f3304d4bde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset.values.reshape(-1,len(columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d5c65abe-2e88-4e4f-b5ed-c9608475bd7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3544, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.468771625943673e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[6,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.20668100e-05],\n",
       "       [6.12095832e-05],\n",
       "       [5.69611031e-05],\n",
       "       ...,\n",
       "       [5.46830366e-01],\n",
       "       [6.09362486e-01],\n",
       "       [6.04357304e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset[0:3]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.16111653e-05],\n",
       "       [4.53171207e-05],\n",
       "       [4.27995029e-05]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3:6,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b74b3790-cf15-4f84-a74c-9ec5cc66993c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Process data (Making sequences)\n",
    "\n",
    "here we split and process data before training.\n",
    "\n",
    "LSTM layer as an input layer expects the data to be 3 dimensions, we will use 'process_data' function to split data into sequences of a fixed length (rnn_size).\n",
    "\n",
    "The neural network is expecting to have an input's shap of [batch_size, rnn_size, nb_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eab8ecfd-c5cb-4d91-b320-73dbe947fc45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split datatest into data_train, data_valid and data_test\n",
    "data_train = dataset[:2900]\n",
    "data_valid = dataset[2900:-30]\n",
    "data_test = dataset[-30:]\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2f8394d7-cfad-477d-80c2-0066e5fb497e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_data(data, rnn_size=3, target_id=0, columns_size=len(columns)):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data)-rnn_size):\n",
    "        X.append(data[i:i+rnn_size,:])\n",
    "        y.append(data[i+rnn_size,0])\n",
    "    return np.array(X).astype(np.float32).reshape((-1,rnn_size,columns_size)), np.array(y).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fc2c7dca-1312-497b-bbd8-16453b64bea0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# process data for RNN\n",
    "X_train, y_train = process_data(data_train)\n",
    "X_val, y_val = process_data(data_valid)\n",
    "X_test, y_test = process_data(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4a266b27-b5e1-4426-854f-42147adfd8f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 3, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4acf7c9c-29e0-45db-8b15-a1b7be2b41b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(611, 3, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dad477a5-9062-48aa-bac5-de48a8ce18b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 3, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af13f9a7-4de8-4212-afc0-f822f2ad1a6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Deep Learning Model\n",
    "\n",
    "Here we initialize the model using Keras.\n",
    "\n",
    "Here we propose to code a basic neural network LSTM + Dense, but you are free to use any architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(3,1)))\n",
    "#model.add(layers.Embedding(input_dim=3, output_dim=1))\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dense(1,activation ='linear', input_shape=(1,0)))\n",
    "model.summary()\n",
    "\n",
    "mse = keras.losses.MeanSquaredError()\n",
    "model.compile(loss='mse',optimizer='rmsprop')\n",
    "\n",
    "#Fitting the Recurrent Neural Network\n",
    "model.fit(x = X_train, y = y_train, validation_data = (X_val, y_val),\n",
    "          batch_size = 32, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04872581], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#root mean squared error\n",
    "def rmse(a,b):\n",
    "    rmse = 0\n",
    "    for i in range(len(a)):\n",
    "        rmse += (a[i]-b[i])**2\n",
    "    return np.sqrt(rmse/len(a))\n",
    "\n",
    "#mean absolute error\n",
    "def mae(a,b):\n",
    "    mae = 0\n",
    "    for i in range(len(a)):\n",
    "        mae += abs(a[i]-b[i])\n",
    "    return mae/len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc040557-528b-4833-8d5b-089d236d9d77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 3\n",
    "\n",
    "def evaluation(model,X_test):\n",
    "    # compute prediction for test\n",
    "    y_pred = model(X_test)\n",
    "    # compute rmse for test\n",
    "    y_pred_inverse = scaler.inverse_transform(np.concatenate([y_pred, data_test[-len(y_pred):,1:]], axis=1))\n",
    "    y_test_inverse = scaler.inverse_transform(data_test.reshape(-1,len(columns)))[rnn_size:]\n",
    "    rmse_score = np.array(rmse(y_pred_inverse,y_test_inverse ))\n",
    "    mae_score = np.array(mae(y_pred_inverse,y_test_inverse ))\n",
    "    print(\"rmse score : \", rmse_score)\n",
    "    print(\"mae score : \", mae_score)\n",
    "    return rmse_score, mae_score\n",
    "    #Graphs for predicted values\n",
    "    plt.plot(y_test_inverse[rnn_size:,0], color = 'red', label = 'true BTC price')\n",
    "    plt.plot(y_pred_inverse[:,0], color = 'blue', label = 'predicted BTC price')\n",
    "    plt.title('BTC price Prediction')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('BTC price')\n",
    "    plt.legend()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "393be024-3cbf-4397-a04b-ae8faf79e229",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# If you get this far, you can : \n",
    "- Test other neural network models\n",
    "- Test other optimizers\n",
    "- Compare results between Arima and RNN models\n",
    "- Find a way to choose most important variables\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of others neural network models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 169,760\n",
      "Trainable params: 169,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 169,793\n",
      "Trainable params: 169,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(None, 3, 128)\n",
      "(None, 3, 128)\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 3s 1ms/sample - loss: 5.5923e-04 - val_loss: 0.0041\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 237us/sample - loss: 1.6875e-04 - val_loss: 7.1213e-04\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 246us/sample - loss: 1.3195e-04 - val_loss: 8.2543e-04\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 232us/sample - loss: 1.2376e-04 - val_loss: 6.7856e-04\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 237us/sample - loss: 1.2394e-04 - val_loss: 5.5860e-04\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 245us/sample - loss: 1.0313e-04 - val_loss: 0.0039\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 243us/sample - loss: 1.0318e-04 - val_loss: 5.4029e-04\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 249us/sample - loss: 9.1485e-05 - val_loss: 0.0012\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 246us/sample - loss: 8.9383e-05 - val_loss: 5.4889e-04\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 255us/sample - loss: 9.3742e-05 - val_loss: 0.0016\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 252us/sample - loss: 7.5552e-05 - val_loss: 0.0012\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 291us/sample - loss: 7.9216e-05 - val_loss: 0.0023\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 270us/sample - loss: 7.7607e-05 - val_loss: 0.0013\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 313us/sample - loss: 7.1609e-05 - val_loss: 8.3650e-04\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 290us/sample - loss: 6.4160e-05 - val_loss: 5.7924e-04\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 280us/sample - loss: 7.5501e-05 - val_loss: 0.0019\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 273us/sample - loss: 6.7003e-05 - val_loss: 5.8578e-04\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 276us/sample - loss: 6.6752e-05 - val_loss: 8.5406e-04\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 287us/sample - loss: 5.7913e-05 - val_loss: 7.5540e-04\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 282us/sample - loss: 6.4822e-05 - val_loss: 6.8169e-04\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 288us/sample - loss: 6.3695e-05 - val_loss: 0.0011\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 287us/sample - loss: 5.0337e-05 - val_loss: 0.0065\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 296us/sample - loss: 6.1187e-05 - val_loss: 5.1205e-04\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 303us/sample - loss: 5.7112e-05 - val_loss: 4.6563e-04\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 305us/sample - loss: 4.9504e-05 - val_loss: 0.0055\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 306us/sample - loss: 5.7912e-05 - val_loss: 5.9331e-04\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 318us/sample - loss: 4.6712e-05 - val_loss: 6.7365e-04\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 339us/sample - loss: 5.6976e-05 - val_loss: 9.1529e-04\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 341us/sample - loss: 4.8637e-05 - val_loss: 0.0028\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 331us/sample - loss: 5.2210e-05 - val_loss: 6.5343e-04\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 321us/sample - loss: 4.8327e-05 - val_loss: 0.0026\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 317us/sample - loss: 4.9718e-05 - val_loss: 0.0018\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 325us/sample - loss: 4.9078e-05 - val_loss: 0.0014\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 320us/sample - loss: 4.8373e-05 - val_loss: 0.0014\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 327us/sample - loss: 4.6891e-05 - val_loss: 0.0013\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 326us/sample - loss: 4.0781e-05 - val_loss: 0.0018\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 336us/sample - loss: 4.3925e-05 - val_loss: 7.3868e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 338us/sample - loss: 5.0223e-05 - val_loss: 4.2618e-04\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 333us/sample - loss: 4.0087e-05 - val_loss: 3.7477e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 336us/sample - loss: 4.0823e-05 - val_loss: 4.8869e-04\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 345us/sample - loss: 4.3311e-05 - val_loss: 0.0011\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 344us/sample - loss: 4.1704e-05 - val_loss: 4.1142e-04\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 351us/sample - loss: 4.3522e-05 - val_loss: 4.2091e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 343us/sample - loss: 3.8185e-05 - val_loss: 7.0980e-04\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 342us/sample - loss: 4.3685e-05 - val_loss: 6.1707e-04\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 348us/sample - loss: 4.0820e-05 - val_loss: 0.0017\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 355us/sample - loss: 4.0186e-05 - val_loss: 3.7465e-04\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 356us/sample - loss: 4.2999e-05 - val_loss: 9.9704e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 350us/sample - loss: 3.9765e-05 - val_loss: 4.5589e-04\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 353us/sample - loss: 3.9382e-05 - val_loss: 3.2923e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb6cb92dcd0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.Input(shape=(3,1)))\n",
    "layer_1 = layers.LSTM(128, activation =\"relu\", return_sequences=True)\n",
    "model.add(layer_1)\n",
    "\n",
    "#Dropout permet d'éviter l'overfitting en remplacant certains input par 0 pendant le set de training\n",
    "layer_2 = layers.Dropout(0.3)\n",
    "model.add(layer_2)\n",
    "\n",
    "layer_3 = layers.GRU(128, activation =\"relu\")\n",
    "model.add(layer_3)\n",
    "\n",
    "#layer_2_bis = layers.Dropout(0.1)\n",
    "#model.add(layer_2_bis)\n",
    "\n",
    "layer_4 = layers.Dense(32,activation ='relu')\n",
    "model.add(layer_4)\n",
    "model.summary()\n",
    "\n",
    "layer_5 = layers.Dense(1,activation ='relu', input_shape=(1,0))\n",
    "model.add(layer_5)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "mse = keras.losses.MeanSquaredError()\n",
    "mae = keras.losses.MeanAbsoluteError()\n",
    "model.compile(loss= mse,optimizer='rmsprop')\n",
    "\n",
    "print(layer_2.input_shape)\n",
    "print(layer_2.output_shape)\n",
    "\n",
    "#Fitting the Recurrent Neural Network\n",
    "model.fit(x = X_train, y = y_train, validation_data = (X_val, y_val),batch_size = 32, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse score :  [3075.02409174]\n",
      "mae score :  2396.864501953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3075.024091742629"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(model,X_test)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 169,760\n",
      "Trainable params: 169,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 3s 1ms/sample - loss: 5.5422e-04 - val_loss: 0.0043\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 216us/sample - loss: 1.2335e-04 - val_loss: 7.1703e-04\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 213us/sample - loss: 9.2315e-05 - val_loss: 0.0011\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 210us/sample - loss: 9.4002e-05 - val_loss: 9.0387e-04\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 215us/sample - loss: 8.4003e-05 - val_loss: 6.6676e-04\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 208us/sample - loss: 8.2183e-05 - val_loss: 5.7946e-04\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 245us/sample - loss: 6.9244e-05 - val_loss: 5.8495e-04\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 235us/sample - loss: 7.3598e-05 - val_loss: 6.1196e-04\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 223us/sample - loss: 6.5579e-05 - val_loss: 0.0037\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 255us/sample - loss: 7.1083e-05 - val_loss: 0.0014\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 246us/sample - loss: 6.1731e-05 - val_loss: 0.0012\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 241us/sample - loss: 5.7245e-05 - val_loss: 8.4894e-04\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 233us/sample - loss: 6.3075e-05 - val_loss: 0.0011\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 231us/sample - loss: 5.7171e-05 - val_loss: 7.1334e-04\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 289us/sample - loss: 5.4317e-05 - val_loss: 0.0015\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 240us/sample - loss: 5.2767e-05 - val_loss: 6.6974e-04\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 241us/sample - loss: 5.3902e-05 - val_loss: 6.0381e-04\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 251us/sample - loss: 4.6847e-05 - val_loss: 9.1050e-04\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 249us/sample - loss: 4.7505e-05 - val_loss: 0.0014\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 249us/sample - loss: 4.7240e-05 - val_loss: 0.0011\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 252us/sample - loss: 4.7547e-05 - val_loss: 0.0021\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 255us/sample - loss: 4.6545e-05 - val_loss: 8.1272e-04\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 257us/sample - loss: 4.8254e-05 - val_loss: 0.0025\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 255us/sample - loss: 4.3263e-05 - val_loss: 8.9043e-04\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 266us/sample - loss: 4.4087e-05 - val_loss: 0.0020\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 269us/sample - loss: 4.2671e-05 - val_loss: 7.1919e-04\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 270us/sample - loss: 4.3523e-05 - val_loss: 8.9255e-04\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 276us/sample - loss: 4.1344e-05 - val_loss: 0.0024\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 304us/sample - loss: 4.0173e-05 - val_loss: 0.0015\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 336us/sample - loss: 4.2578e-05 - val_loss: 7.5674e-04\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 335us/sample - loss: 4.0936e-05 - val_loss: 0.0011\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 340us/sample - loss: 3.6303e-05 - val_loss: 5.8920e-04\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 318us/sample - loss: 3.7246e-05 - val_loss: 9.8783e-04\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 329us/sample - loss: 4.0365e-05 - val_loss: 0.0011\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 338us/sample - loss: 3.5659e-05 - val_loss: 0.0022\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 307us/sample - loss: 3.5555e-05 - val_loss: 4.8621e-04\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 302us/sample - loss: 3.6766e-05 - val_loss: 0.0013\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 302us/sample - loss: 3.5024e-05 - val_loss: 8.4201e-04\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 334us/sample - loss: 3.2128e-05 - val_loss: 0.0020\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 340us/sample - loss: 3.8640e-05 - val_loss: 6.3197e-04\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 355us/sample - loss: 3.2634e-05 - val_loss: 4.7161e-04\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 352us/sample - loss: 3.3490e-05 - val_loss: 0.0016\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 329us/sample - loss: 2.9565e-05 - val_loss: 9.3156e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 339us/sample - loss: 3.3708e-05 - val_loss: 4.1311e-04\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 329us/sample - loss: 2.9211e-05 - val_loss: 5.3515e-04\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 342us/sample - loss: 2.9428e-05 - val_loss: 3.8798e-04\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 321us/sample - loss: 3.0393e-05 - val_loss: 5.5112e-04\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 336us/sample - loss: 3.0900e-05 - val_loss: 3.5566e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 332us/sample - loss: 2.8151e-05 - val_loss: 3.8258e-04\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 342us/sample - loss: 2.9603e-05 - val_loss: 0.0016\n",
      "rmse score :  [6049.9002743]\n",
      "mae score :  5320.20849609375\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 169,760\n",
      "Trainable params: 169,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 5s 2ms/sample - loss: 4.1929e-04 - val_loss: 0.0047\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 409us/sample - loss: 1.2932e-04 - val_loss: 0.0028\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 367us/sample - loss: 1.1396e-04 - val_loss: 0.0025\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 360us/sample - loss: 9.2833e-05 - val_loss: 0.0014\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 368us/sample - loss: 1.0493e-04 - val_loss: 0.0017\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 349us/sample - loss: 9.1368e-05 - val_loss: 0.0016\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 340us/sample - loss: 7.7549e-05 - val_loss: 0.0036\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 343us/sample - loss: 8.1091e-05 - val_loss: 0.0014\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 344us/sample - loss: 7.4365e-05 - val_loss: 0.0016\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 351us/sample - loss: 7.3583e-05 - val_loss: 5.2660e-04\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 345us/sample - loss: 6.3211e-05 - val_loss: 5.0876e-04\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 342us/sample - loss: 6.7714e-05 - val_loss: 5.2397e-04\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 354us/sample - loss: 6.5191e-05 - val_loss: 5.5609e-04\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 360us/sample - loss: 6.0482e-05 - val_loss: 0.0019\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 348us/sample - loss: 6.3191e-05 - val_loss: 6.3699e-04\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 354us/sample - loss: 5.6193e-05 - val_loss: 6.1434e-04\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 350us/sample - loss: 5.4392e-05 - val_loss: 6.4682e-04\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 349us/sample - loss: 5.4955e-05 - val_loss: 6.6844e-04\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 351us/sample - loss: 5.2186e-05 - val_loss: 0.0021\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 348us/sample - loss: 4.9375e-05 - val_loss: 0.0015\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 347us/sample - loss: 4.7830e-05 - val_loss: 0.0011\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 354us/sample - loss: 4.3779e-05 - val_loss: 0.0010\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 363us/sample - loss: 4.6272e-05 - val_loss: 0.0026\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 353us/sample - loss: 4.5983e-05 - val_loss: 5.4456e-04\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 356us/sample - loss: 4.6777e-05 - val_loss: 0.0015\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 357us/sample - loss: 4.4852e-05 - val_loss: 4.5888e-04\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 355us/sample - loss: 4.3847e-05 - val_loss: 0.0012\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 356us/sample - loss: 4.3032e-05 - val_loss: 4.6373e-04\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 360us/sample - loss: 3.9679e-05 - val_loss: 4.8622e-04\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 362us/sample - loss: 3.7138e-05 - val_loss: 5.7052e-04\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 361us/sample - loss: 3.7195e-05 - val_loss: 6.7916e-04\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 358us/sample - loss: 3.4925e-05 - val_loss: 4.9029e-04\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 363us/sample - loss: 3.8698e-05 - val_loss: 4.7867e-04\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 357us/sample - loss: 3.7955e-05 - val_loss: 0.0031\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 361us/sample - loss: 3.7178e-05 - val_loss: 4.9551e-04\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 380us/sample - loss: 3.2502e-05 - val_loss: 6.2748e-04\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 365us/sample - loss: 3.5893e-05 - val_loss: 7.8204e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 3.5210e-05 - val_loss: 6.5755e-04\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 370us/sample - loss: 3.3539e-05 - val_loss: 5.2325e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 364us/sample - loss: 3.1470e-05 - val_loss: 0.0012\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 363us/sample - loss: 3.0177e-05 - val_loss: 6.4225e-04\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 367us/sample - loss: 3.3671e-05 - val_loss: 6.4569e-04\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 368us/sample - loss: 3.3809e-05 - val_loss: 5.3472e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 363us/sample - loss: 3.0387e-05 - val_loss: 0.0013\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 371us/sample - loss: 3.1962e-05 - val_loss: 4.1442e-04\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 366us/sample - loss: 3.1174e-05 - val_loss: 4.2176e-04\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 421us/sample - loss: 2.7999e-05 - val_loss: 7.0391e-04\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 466us/sample - loss: 2.9410e-05 - val_loss: 0.0040\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 438us/sample - loss: 3.1262e-05 - val_loss: 4.9777e-04\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 3.0169e-05 - val_loss: 3.0191e-04\n",
      "rmse score :  [2841.93087087]\n",
      "mae score :  2172.759521484375\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_18 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 169,760\n",
      "Trainable params: 169,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 4s 1ms/sample - loss: 5.7022e-04 - val_loss: 0.0116\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 421us/sample - loss: 1.2867e-04 - val_loss: 0.0045\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 459us/sample - loss: 1.1110e-04 - val_loss: 0.0012\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 441us/sample - loss: 9.5646e-05 - val_loss: 8.4728e-04\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 441us/sample - loss: 8.4759e-05 - val_loss: 0.0053\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 411us/sample - loss: 9.3213e-05 - val_loss: 0.0030\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 7.9787e-05 - val_loss: 6.5842e-04\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 455us/sample - loss: 8.7505e-05 - val_loss: 0.0014\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 451us/sample - loss: 6.8831e-05 - val_loss: 0.0018\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 7.3723e-05 - val_loss: 0.0022\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 480us/sample - loss: 6.9542e-05 - val_loss: 0.0027\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 458us/sample - loss: 6.6717e-05 - val_loss: 5.7680e-04\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 458us/sample - loss: 6.0759e-05 - val_loss: 6.6545e-04\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 6.4645e-05 - val_loss: 7.5364e-04\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 6.3205e-05 - val_loss: 6.7636e-04\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 399us/sample - loss: 5.8661e-05 - val_loss: 6.4795e-04\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 402us/sample - loss: 6.0584e-05 - val_loss: 7.4081e-04\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 399us/sample - loss: 6.0070e-05 - val_loss: 0.0023\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2897/2897 [==============================] - 1s 391us/sample - loss: 5.9955e-05 - val_loss: 0.0028\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 378us/sample - loss: 5.4268e-05 - val_loss: 9.4148e-04\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 388us/sample - loss: 5.1847e-05 - val_loss: 0.0013\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 386us/sample - loss: 5.0568e-05 - val_loss: 9.1948e-04\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 388us/sample - loss: 4.9203e-05 - val_loss: 6.4317e-04\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 404us/sample - loss: 5.2263e-05 - val_loss: 6.8503e-04\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 417us/sample - loss: 5.0204e-05 - val_loss: 7.8043e-04\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 4.7731e-05 - val_loss: 5.7977e-04\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 4.7213e-05 - val_loss: 9.1111e-04\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 390us/sample - loss: 4.6604e-05 - val_loss: 0.0014\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 391us/sample - loss: 4.2138e-05 - val_loss: 5.1681e-04\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 390us/sample - loss: 4.8996e-05 - val_loss: 6.4989e-04\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 427us/sample - loss: 4.9012e-05 - val_loss: 0.0019\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 480us/sample - loss: 4.3448e-05 - val_loss: 6.9407e-04\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 511us/sample - loss: 4.1504e-05 - val_loss: 5.0314e-04\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 438us/sample - loss: 4.2399e-05 - val_loss: 0.0010\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 394us/sample - loss: 4.1592e-05 - val_loss: 8.5108e-04\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 429us/sample - loss: 4.0807e-05 - val_loss: 0.0017\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 450us/sample - loss: 3.7521e-05 - val_loss: 4.8633e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 457us/sample - loss: 4.2167e-05 - val_loss: 0.0012\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 432us/sample - loss: 4.3472e-05 - val_loss: 7.9728e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 421us/sample - loss: 4.0966e-05 - val_loss: 6.4401e-04\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 397us/sample - loss: 3.6197e-05 - val_loss: 7.5564e-04\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 401us/sample - loss: 3.7583e-05 - val_loss: 6.4749e-04\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 395us/sample - loss: 3.6631e-05 - val_loss: 4.6583e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 402us/sample - loss: 3.9827e-05 - val_loss: 5.3693e-04\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 4.2338e-05 - val_loss: 0.0011\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 394us/sample - loss: 3.7624e-05 - val_loss: 0.0030\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 419us/sample - loss: 3.7189e-05 - val_loss: 5.1317e-04\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 399us/sample - loss: 3.5787e-05 - val_loss: 6.3676e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 497us/sample - loss: 3.6200e-05 - val_loss: 3.8263e-04\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 446us/sample - loss: 3.5646e-05 - val_loss: 4.3884e-04\n",
      "rmse score :  [2714.94141954]\n",
      "mae score :  2050.45166015625\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_19 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 169,760\n",
      "Trainable params: 169,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 5s 2ms/sample - loss: 5.2935e-04 - val_loss: 0.0044\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 483us/sample - loss: 1.7129e-04 - val_loss: 0.0011\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 446us/sample - loss: 1.2546e-04 - val_loss: 9.5947e-04\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 408us/sample - loss: 1.3134e-04 - val_loss: 0.0024\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 444us/sample - loss: 1.1917e-04 - val_loss: 8.7716e-04\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 440us/sample - loss: 1.0494e-04 - val_loss: 0.0016\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 426us/sample - loss: 1.0022e-04 - val_loss: 4.9029e-04\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 409us/sample - loss: 9.0974e-05 - val_loss: 8.0429e-04\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 443us/sample - loss: 8.9536e-05 - val_loss: 7.5255e-04\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 482us/sample - loss: 9.4431e-05 - val_loss: 0.0032\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 498us/sample - loss: 8.6040e-05 - val_loss: 8.4252e-04\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 486us/sample - loss: 7.9815e-05 - val_loss: 8.0863e-04\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 462us/sample - loss: 7.0965e-05 - val_loss: 8.3646e-04\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 7.7681e-05 - val_loss: 0.0017\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 430us/sample - loss: 7.3922e-05 - val_loss: 0.0015\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 6.9827e-05 - val_loss: 0.0016\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 437us/sample - loss: 6.2596e-05 - val_loss: 0.0012\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 452us/sample - loss: 7.0898e-05 - val_loss: 7.9247e-04\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 6.5467e-05 - val_loss: 5.0030e-04\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 432us/sample - loss: 6.5878e-05 - val_loss: 0.0029\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 454us/sample - loss: 5.7858e-05 - val_loss: 6.7777e-04\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 477us/sample - loss: 5.8205e-05 - val_loss: 7.6621e-04\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 472us/sample - loss: 5.6923e-05 - val_loss: 8.0253e-04\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 422us/sample - loss: 5.7405e-05 - val_loss: 8.6068e-04\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 412us/sample - loss: 5.8224e-05 - val_loss: 8.0542e-04\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 467us/sample - loss: 5.6359e-05 - val_loss: 0.0011\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 456us/sample - loss: 5.3029e-05 - val_loss: 0.0014\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 479us/sample - loss: 5.4161e-05 - val_loss: 4.9163e-04\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 448us/sample - loss: 4.7231e-05 - val_loss: 7.0023e-04\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 5.1817e-05 - val_loss: 0.0013\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 5.1899e-05 - val_loss: 9.6809e-04\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 401us/sample - loss: 5.0752e-05 - val_loss: 0.0016\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 5.2788e-05 - val_loss: 4.4378e-04\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 449us/sample - loss: 4.5896e-05 - val_loss: 0.0048\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 465us/sample - loss: 4.7167e-05 - val_loss: 0.0011\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 446us/sample - loss: 4.9022e-05 - val_loss: 8.1340e-04\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 475us/sample - loss: 4.6806e-05 - val_loss: 4.5937e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 494us/sample - loss: 4.2676e-05 - val_loss: 6.7325e-04\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 426us/sample - loss: 4.4823e-05 - val_loss: 7.1275e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 457us/sample - loss: 4.1987e-05 - val_loss: 7.0819e-04\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 426us/sample - loss: 4.3278e-05 - val_loss: 6.3306e-04\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 465us/sample - loss: 4.5680e-05 - val_loss: 8.1174e-04\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 434us/sample - loss: 4.4633e-05 - val_loss: 8.9335e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 421us/sample - loss: 4.3880e-05 - val_loss: 3.9415e-04\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 395us/sample - loss: 4.3105e-05 - val_loss: 8.9761e-04\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 399us/sample - loss: 4.3582e-05 - val_loss: 0.0013\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 399us/sample - loss: 4.1792e-05 - val_loss: 4.2625e-04\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 398us/sample - loss: 4.1788e-05 - val_loss: 5.7762e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 401us/sample - loss: 4.0175e-05 - val_loss: 4.5761e-04\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 400us/sample - loss: 4.0333e-05 - val_loss: 3.5403e-04\n",
      "rmse score :  [2981.04606166]\n",
      "mae score :  2327.816650390625\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_25 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_20 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 169,760\n",
      "Trainable params: 169,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 5s 2ms/sample - loss: 9.6751e-04 - val_loss: 0.0112\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 410us/sample - loss: 1.7317e-04 - val_loss: 0.0041\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 397us/sample - loss: 1.0887e-04 - val_loss: 0.0026\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 390us/sample - loss: 1.1718e-04 - val_loss: 0.0020\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 1.0817e-04 - val_loss: 0.0025\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 452us/sample - loss: 1.0571e-04 - val_loss: 9.9093e-04\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 418us/sample - loss: 9.3664e-05 - val_loss: 5.2203e-04\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 398us/sample - loss: 9.8192e-05 - val_loss: 6.2524e-04\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 9.2935e-05 - val_loss: 0.0063\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 400us/sample - loss: 8.1171e-05 - val_loss: 0.0056\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 406us/sample - loss: 8.2816e-05 - val_loss: 7.3550e-04\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 423us/sample - loss: 8.8185e-05 - val_loss: 0.0036\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 461us/sample - loss: 7.2356e-05 - val_loss: 0.0020\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 412us/sample - loss: 7.0121e-05 - val_loss: 5.6430e-04\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 405us/sample - loss: 7.9858e-05 - val_loss: 0.0038\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 410us/sample - loss: 6.7696e-05 - val_loss: 0.0035\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 7.5671e-05 - val_loss: 0.0065\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 397us/sample - loss: 6.2524e-05 - val_loss: 0.0020\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 387us/sample - loss: 7.2612e-05 - val_loss: 0.0010\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 393us/sample - loss: 6.2247e-05 - val_loss: 0.0017\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 397us/sample - loss: 6.3346e-05 - val_loss: 0.0019\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 6.4579e-05 - val_loss: 0.0027\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 5.5653e-05 - val_loss: 0.0016\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 455us/sample - loss: 5.4006e-05 - val_loss: 0.0022\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 397us/sample - loss: 5.4879e-05 - val_loss: 0.0023\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 409us/sample - loss: 5.8423e-05 - val_loss: 0.0017\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 454us/sample - loss: 5.6435e-05 - val_loss: 5.1379e-04\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 5.5948e-05 - val_loss: 5.2418e-04\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 458us/sample - loss: 5.4339e-05 - val_loss: 5.5791e-04\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 467us/sample - loss: 5.4147e-05 - val_loss: 0.0011\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 446us/sample - loss: 5.5060e-05 - val_loss: 9.0456e-04\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 406us/sample - loss: 5.4657e-05 - val_loss: 0.0023\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 5.4473e-05 - val_loss: 0.0020\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 450us/sample - loss: 4.8915e-05 - val_loss: 7.6166e-04\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 429us/sample - loss: 4.7576e-05 - val_loss: 0.0015\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 472us/sample - loss: 5.3543e-05 - val_loss: 4.7422e-04\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 4.9893e-05 - val_loss: 9.4779e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 397us/sample - loss: 4.7914e-05 - val_loss: 4.7754e-04\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 451us/sample - loss: 5.1359e-05 - val_loss: 4.2209e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 463us/sample - loss: 5.0498e-05 - val_loss: 9.9900e-04\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 426us/sample - loss: 4.6372e-05 - val_loss: 0.0032\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 436us/sample - loss: 4.7518e-05 - val_loss: 0.0013\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 504us/sample - loss: 4.4727e-05 - val_loss: 5.1367e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 442us/sample - loss: 5.2902e-05 - val_loss: 0.0024\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 409us/sample - loss: 4.3922e-05 - val_loss: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 401us/sample - loss: 4.3515e-05 - val_loss: 0.0011\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 4.3128e-05 - val_loss: 0.0022\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 4.6064e-05 - val_loss: 9.6105e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 406us/sample - loss: 4.0458e-05 - val_loss: 0.0016\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 383us/sample - loss: 4.6602e-05 - val_loss: 6.6421e-04\n",
      "rmse score :  [3066.31373657]\n",
      "mae score :  2543.991943359375\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_26 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_21 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 169,760\n",
      "Trainable params: 169,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 4s 1ms/sample - loss: 5.0995e-04 - val_loss: 0.0140\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 397us/sample - loss: 1.7630e-04 - val_loss: 0.0100\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 387us/sample - loss: 1.5717e-04 - val_loss: 0.0045\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 390us/sample - loss: 1.3966e-04 - val_loss: 0.0017\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 390us/sample - loss: 1.2574e-04 - val_loss: 5.7771e-04\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 382us/sample - loss: 1.1899e-04 - val_loss: 0.0023\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 388us/sample - loss: 1.0133e-04 - val_loss: 0.0013\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 381us/sample - loss: 1.1180e-04 - val_loss: 0.0023\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 387us/sample - loss: 9.1076e-05 - val_loss: 0.0016\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 380us/sample - loss: 9.2285e-05 - val_loss: 0.0015\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 9.1811e-05 - val_loss: 0.0033\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 383us/sample - loss: 8.9193e-05 - val_loss: 0.0019\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 8.4146e-05 - val_loss: 0.0024\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 392us/sample - loss: 7.6977e-05 - val_loss: 0.0012\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 382us/sample - loss: 7.1110e-05 - val_loss: 9.1271e-04\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 387us/sample - loss: 7.8033e-05 - val_loss: 0.0022\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 386us/sample - loss: 7.3943e-05 - val_loss: 0.0010\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 383us/sample - loss: 7.1786e-05 - val_loss: 0.0027\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 436us/sample - loss: 6.6279e-05 - val_loss: 0.0022\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 403us/sample - loss: 7.0481e-05 - val_loss: 6.9358e-04\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 7.0020e-05 - val_loss: 0.0014\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 404us/sample - loss: 6.3510e-05 - val_loss: 0.0015\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 450us/sample - loss: 7.0336e-05 - val_loss: 0.0011\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 401us/sample - loss: 6.8131e-05 - val_loss: 0.0011\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 405us/sample - loss: 5.5479e-05 - val_loss: 0.0017\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 6.5254e-05 - val_loss: 0.0051\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 412us/sample - loss: 6.0676e-05 - val_loss: 0.0022\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 5.2805e-05 - val_loss: 0.0031\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 463us/sample - loss: 6.1928e-05 - val_loss: 0.0013\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 5.8644e-05 - val_loss: 0.0017\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 412us/sample - loss: 5.4446e-05 - val_loss: 4.2263e-04\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 392us/sample - loss: 5.9257e-05 - val_loss: 6.7611e-04\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 393us/sample - loss: 5.5219e-05 - val_loss: 6.8042e-04\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 430us/sample - loss: 5.6227e-05 - val_loss: 9.5067e-04\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 449us/sample - loss: 5.0477e-05 - val_loss: 0.0010\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 441us/sample - loss: 5.2564e-05 - val_loss: 0.0010\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 439us/sample - loss: 4.7805e-05 - val_loss: 9.2492e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 436us/sample - loss: 4.9591e-05 - val_loss: 0.0021\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 4.8809e-05 - val_loss: 4.1281e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 378us/sample - loss: 5.4076e-05 - val_loss: 0.0028\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 382us/sample - loss: 5.5267e-05 - val_loss: 0.0040\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 381us/sample - loss: 5.0755e-05 - val_loss: 5.7387e-04\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 390us/sample - loss: 4.5441e-05 - val_loss: 5.0930e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 380us/sample - loss: 5.0455e-05 - val_loss: 0.0015\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 389us/sample - loss: 4.7996e-05 - val_loss: 9.2020e-04\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 377us/sample - loss: 5.1492e-05 - val_loss: 0.0013\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 382us/sample - loss: 4.4016e-05 - val_loss: 6.7567e-04\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 382us/sample - loss: 4.5944e-05 - val_loss: 5.5597e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 4.8090e-05 - val_loss: 0.0026\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 471us/sample - loss: 4.9692e-05 - val_loss: 0.0012\n",
      "rmse score :  [4176.01393124]\n",
      "mae score :  3559.778564453125\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_22 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 169,760\n",
      "Trainable params: 169,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "2897/2897 [==============================] - 4s 2ms/sample - loss: 5.2018e-04 - val_loss: 0.0182\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 427us/sample - loss: 2.0241e-04 - val_loss: 0.0071\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 1.7059e-04 - val_loss: 0.0020\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 473us/sample - loss: 1.6488e-04 - val_loss: 0.0031\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 450us/sample - loss: 1.3302e-04 - val_loss: 0.0023\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 442us/sample - loss: 1.2701e-04 - val_loss: 0.0068\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 493us/sample - loss: 1.1888e-04 - val_loss: 0.0025\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 455us/sample - loss: 1.1451e-04 - val_loss: 9.1377e-04\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 1.0430e-04 - val_loss: 0.0019\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 384us/sample - loss: 1.0922e-04 - val_loss: 0.0018\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 388us/sample - loss: 9.3191e-05 - val_loss: 0.0012\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 448us/sample - loss: 8.7452e-05 - val_loss: 0.0017\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 430us/sample - loss: 9.3685e-05 - val_loss: 0.0030\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 409us/sample - loss: 8.5671e-05 - val_loss: 0.0016\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 427us/sample - loss: 8.3441e-05 - val_loss: 0.0073\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 448us/sample - loss: 8.9362e-05 - val_loss: 0.0042\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 429us/sample - loss: 8.1927e-05 - val_loss: 0.0017\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 429us/sample - loss: 7.6808e-05 - val_loss: 0.0015\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 438us/sample - loss: 8.4235e-05 - val_loss: 0.0023\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 454us/sample - loss: 7.4002e-05 - val_loss: 0.0022\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 462us/sample - loss: 6.7115e-05 - val_loss: 0.0015\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 473us/sample - loss: 6.7970e-05 - val_loss: 0.0013\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 442us/sample - loss: 6.9339e-05 - val_loss: 0.0034\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 419us/sample - loss: 6.7847e-05 - val_loss: 0.0027\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 408us/sample - loss: 6.5286e-05 - val_loss: 0.0075\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 457us/sample - loss: 6.9392e-05 - val_loss: 0.0036\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 475us/sample - loss: 6.1876e-05 - val_loss: 0.0063\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 470us/sample - loss: 6.9710e-05 - val_loss: 0.0048\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 465us/sample - loss: 6.5911e-05 - val_loss: 0.0019\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 468us/sample - loss: 5.8981e-05 - val_loss: 0.0043\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 446us/sample - loss: 5.8674e-05 - val_loss: 0.0012\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 503us/sample - loss: 6.3438e-05 - val_loss: 0.0033\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 497us/sample - loss: 5.3261e-05 - val_loss: 0.0031\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 505us/sample - loss: 6.0262e-05 - val_loss: 0.0043\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 471us/sample - loss: 5.8908e-05 - val_loss: 0.0051\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 463us/sample - loss: 5.7362e-05 - val_loss: 0.0042\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 448us/sample - loss: 5.0191e-05 - val_loss: 0.0014\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 474us/sample - loss: 5.4427e-05 - val_loss: 0.0031\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 498us/sample - loss: 5.8923e-05 - val_loss: 0.0056\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 465us/sample - loss: 5.7891e-05 - val_loss: 0.0015\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 470us/sample - loss: 4.8098e-05 - val_loss: 0.0042\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 423us/sample - loss: 5.3657e-05 - val_loss: 0.0040\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 5.2673e-05 - val_loss: 0.0043\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 397us/sample - loss: 5.5185e-05 - val_loss: 0.0068\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 456us/sample - loss: 5.1781e-05 - val_loss: 0.0047\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 412us/sample - loss: 5.2479e-05 - val_loss: 0.0021\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 393us/sample - loss: 4.8324e-05 - val_loss: 0.0043\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 395us/sample - loss: 4.9462e-05 - val_loss: 0.0035\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 393us/sample - loss: 5.0204e-05 - val_loss: 0.0066\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 395us/sample - loss: 4.9033e-05 - val_loss: 0.0034\n",
      "rmse score :  [7683.41989225]\n",
      "mae score :  6814.50341796875\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_23 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 169,760\n",
      "Trainable params: 169,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "2897/2897 [==============================] - 5s 2ms/sample - loss: 5.1002e-04 - val_loss: 0.0014\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 446us/sample - loss: 1.8990e-04 - val_loss: 0.0012\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 458us/sample - loss: 1.5604e-04 - val_loss: 7.8236e-04\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 1.5368e-04 - val_loss: 0.0015\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 409us/sample - loss: 1.5253e-04 - val_loss: 0.0031\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 389us/sample - loss: 1.3166e-04 - val_loss: 7.7871e-04\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 384us/sample - loss: 1.2958e-04 - val_loss: 0.0061\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 387us/sample - loss: 1.0205e-04 - val_loss: 0.0024\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 400us/sample - loss: 9.9005e-05 - val_loss: 8.7862e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 383us/sample - loss: 1.0032e-04 - val_loss: 0.0055\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 9.7637e-05 - val_loss: 0.0063\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 408us/sample - loss: 8.9395e-05 - val_loss: 0.0068\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 394us/sample - loss: 9.5893e-05 - val_loss: 0.0041\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 394us/sample - loss: 8.5503e-05 - val_loss: 0.0058\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 410us/sample - loss: 8.6677e-05 - val_loss: 0.0053\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 381us/sample - loss: 8.4932e-05 - val_loss: 0.0036\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 420us/sample - loss: 8.1704e-05 - val_loss: 0.0033\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 8.4844e-05 - val_loss: 0.0027\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 394us/sample - loss: 7.9283e-05 - val_loss: 0.0055\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 387us/sample - loss: 7.6175e-05 - val_loss: 0.0095\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 382us/sample - loss: 8.0547e-05 - val_loss: 0.0087\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 382us/sample - loss: 7.2298e-05 - val_loss: 0.0062\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 378us/sample - loss: 6.6185e-05 - val_loss: 0.0062\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 374us/sample - loss: 6.4380e-05 - val_loss: 0.0044\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 419us/sample - loss: 7.1869e-05 - val_loss: 0.0096\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 388us/sample - loss: 6.4573e-05 - val_loss: 0.0015\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 374us/sample - loss: 7.3146e-05 - val_loss: 0.0035\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 381us/sample - loss: 6.2896e-05 - val_loss: 0.0025\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 7.1753e-05 - val_loss: 0.0090\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 430us/sample - loss: 6.6594e-05 - val_loss: 0.0047\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 454us/sample - loss: 5.8715e-05 - val_loss: 0.0085\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 434us/sample - loss: 5.9502e-05 - val_loss: 0.0036\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 392us/sample - loss: 5.8822e-05 - val_loss: 0.0049\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 417us/sample - loss: 7.0330e-05 - val_loss: 0.0077\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 413us/sample - loss: 5.6858e-05 - val_loss: 0.0073\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 6.5323e-05 - val_loss: 0.0067\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 400us/sample - loss: 5.7158e-05 - val_loss: 0.0095\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 397us/sample - loss: 6.0754e-05 - val_loss: 0.0066\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 391us/sample - loss: 5.5220e-05 - val_loss: 0.0064\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 393us/sample - loss: 6.2454e-05 - val_loss: 0.0052\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 383us/sample - loss: 5.2294e-05 - val_loss: 0.0060\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 384us/sample - loss: 6.5405e-05 - val_loss: 0.0052\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 5.6204e-05 - val_loss: 0.0091\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 390us/sample - loss: 5.7769e-05 - val_loss: 0.0074\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 5.7316e-05 - val_loss: 0.0051\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 386us/sample - loss: 5.2607e-05 - val_loss: 0.0054\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 382us/sample - loss: 5.6364e-05 - val_loss: 0.0038\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 386us/sample - loss: 5.5478e-05 - val_loss: 0.0044\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 390us/sample - loss: 5.1019e-05 - val_loss: 0.0088\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 5.4067e-05 - val_loss: 0.0123\n",
      "rmse score :  [15348.76337457]\n",
      "mae score :  14535.78515625\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 3, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_24 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 169,760\n",
      "Trainable params: 169,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 4s 1ms/sample - loss: 5.8741e-04 - val_loss: 0.0081\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 392us/sample - loss: 2.4762e-04 - val_loss: 0.0045\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 391us/sample - loss: 2.2389e-04 - val_loss: 0.0099\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 392us/sample - loss: 1.9104e-04 - val_loss: 0.0095\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 381us/sample - loss: 1.6225e-04 - val_loss: 0.0076\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 383us/sample - loss: 1.5318e-04 - val_loss: 0.0071\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 391us/sample - loss: 1.4713e-04 - val_loss: 0.0125\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 383us/sample - loss: 1.2644e-04 - val_loss: 0.0094\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 386us/sample - loss: 1.4464e-04 - val_loss: 0.0188\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 1.2600e-04 - val_loss: 0.0116\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 443us/sample - loss: 1.2431e-04 - val_loss: 0.0243\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 408us/sample - loss: 1.0631e-04 - val_loss: 0.0297\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 391us/sample - loss: 1.1650e-04 - val_loss: 0.0173\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 1.0543e-04 - val_loss: 0.0163\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 405us/sample - loss: 1.0529e-04 - val_loss: 0.0170\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 384us/sample - loss: 1.0003e-04 - val_loss: 0.0245\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 393us/sample - loss: 9.5840e-05 - val_loss: 0.0252\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 393us/sample - loss: 8.8185e-05 - val_loss: 0.0176\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 388us/sample - loss: 9.1687e-05 - val_loss: 0.0180\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 386us/sample - loss: 8.4019e-05 - val_loss: 0.0203\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 388us/sample - loss: 8.3654e-05 - val_loss: 0.0179\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 392us/sample - loss: 8.0743e-05 - val_loss: 0.0253\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 391us/sample - loss: 8.4233e-05 - val_loss: 0.0199\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 387us/sample - loss: 7.4869e-05 - val_loss: 0.0153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 378us/sample - loss: 8.4100e-05 - val_loss: 0.0218\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 370us/sample - loss: 8.1509e-05 - val_loss: 0.0147\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 367us/sample - loss: 7.7103e-05 - val_loss: 0.0165\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 395us/sample - loss: 7.2024e-05 - val_loss: 0.0189\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 423us/sample - loss: 7.9023e-05 - val_loss: 0.0181\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 457us/sample - loss: 6.8823e-05 - val_loss: 0.0125\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 452us/sample - loss: 6.7839e-05 - val_loss: 0.0159\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 402us/sample - loss: 7.3823e-05 - val_loss: 0.0252\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 387us/sample - loss: 6.7353e-05 - val_loss: 0.0204\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 409us/sample - loss: 6.3697e-05 - val_loss: 0.0192\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 397us/sample - loss: 7.0277e-05 - val_loss: 0.0245\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 393us/sample - loss: 6.5265e-05 - val_loss: 0.0239\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 383us/sample - loss: 5.9310e-05 - val_loss: 0.0240\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 382us/sample - loss: 7.0135e-05 - val_loss: 0.0289\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 381us/sample - loss: 7.0903e-05 - val_loss: 0.0219\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 386us/sample - loss: 6.0767e-05 - val_loss: 0.0253\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 399us/sample - loss: 6.7114e-05 - val_loss: 0.0239\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 377us/sample - loss: 6.1862e-05 - val_loss: 0.0221\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 388us/sample - loss: 6.2795e-05 - val_loss: 0.0227\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 384us/sample - loss: 6.2773e-05 - val_loss: 0.0261\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 386us/sample - loss: 6.2940e-05 - val_loss: 0.0281\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 386us/sample - loss: 6.6311e-05 - val_loss: 0.0267\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 403us/sample - loss: 6.1789e-05 - val_loss: 0.0291\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 426us/sample - loss: 6.0462e-05 - val_loss: 0.0322\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 5.9600e-05 - val_loss: 0.0283\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 411us/sample - loss: 5.9489e-05 - val_loss: 0.0312\n",
      "rmse score :  [25585.18248998]\n",
      "mae score :  24503.07421875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'dropout values')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def benchmark_dropout(n):\n",
    "    dropout_values = [i/10 for i in range(0,n)]\n",
    "    rmse_score = []\n",
    "    \n",
    "    for q in dropout_values :\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        model.add(keras.Input(shape=(3,1)))\n",
    "        layer_1 = layers.LSTM(128, activation =\"relu\", return_sequences=True)\n",
    "        model.add(layer_1)\n",
    "\n",
    "        #Dropout permet d'éviter l'overfitting en remplacant certains input par 0 pendant le set de training\n",
    "        layer_2 = layers.Dropout(q)\n",
    "        model.add(layer_2)\n",
    "\n",
    "        layer_3 = layers.GRU(128, activation =\"relu\")\n",
    "        model.add(layer_3)\n",
    "\n",
    "        layer_4 = layers.Dense(32,activation ='relu')\n",
    "        model.add(layer_4)\n",
    "        model.summary()\n",
    "\n",
    "        layer_5 = layers.Dense(1,activation ='relu', input_shape=(1,0))\n",
    "        model.add(layer_5)\n",
    "\n",
    "        mse = keras.losses.MeanSquaredError()\n",
    "        mae = keras.losses.MeanAbsoluteError()\n",
    "        model.compile(loss= mse,optimizer='rmsprop')\n",
    "\n",
    "        #Fitting the Recurrent Neural Network\n",
    "        model.fit(x = X_train, y = y_train, validation_data = (X_val, y_val),batch_size = 32, epochs = 50)\n",
    "        \n",
    "        rmse_score.append(evaluation(model,X_test)[0][0])\n",
    "    \n",
    "    return dropout_values, rmse_score\n",
    "\n",
    "dropout_values, rmse_score = benchmark_dropout(9)\n",
    "plt.plot(dropout_values, rmse_score)\n",
    "plt.ylabel(\"rmse score\")\n",
    "plt.xlabel(\"dropout values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dropout_values, rmse_score)\n",
    "plt.ylabel(\"rmse score\")\n",
    "plt.xlabel(\"dropout values\")\n",
    "plt.savefig('rmse_dropout_values.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 3s 1ms/sample - loss: 3.7172e-04 - val_loss: 0.0011\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 231us/sample - loss: 1.2194e-04 - val_loss: 0.0044\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 239us/sample - loss: 1.0065e-04 - val_loss: 0.0016\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 237us/sample - loss: 9.7428e-05 - val_loss: 6.8187e-04\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 241us/sample - loss: 8.8168e-05 - val_loss: 6.7194e-04\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 248us/sample - loss: 8.8690e-05 - val_loss: 6.9433e-04\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 276us/sample - loss: 8.0107e-05 - val_loss: 4.5250e-04\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 251us/sample - loss: 7.8056e-05 - val_loss: 5.2900e-04\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 255us/sample - loss: 6.8833e-05 - val_loss: 4.6437e-04\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 256us/sample - loss: 6.9951e-05 - val_loss: 9.1325e-04\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 261us/sample - loss: 6.2712e-05 - val_loss: 7.4230e-04\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 263us/sample - loss: 6.6059e-05 - val_loss: 6.0231e-04\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 266us/sample - loss: 5.8771e-05 - val_loss: 6.2696e-04\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 333us/sample - loss: 5.7116e-05 - val_loss: 6.4475e-04\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 274us/sample - loss: 5.6528e-05 - val_loss: 0.0025\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 277us/sample - loss: 5.8506e-05 - val_loss: 5.2861e-04\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 284us/sample - loss: 5.4803e-05 - val_loss: 4.1300e-04\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 281us/sample - loss: 4.9696e-05 - val_loss: 0.0021\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 286us/sample - loss: 5.2886e-05 - val_loss: 4.2363e-04\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 290us/sample - loss: 4.7337e-05 - val_loss: 0.0019\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 290us/sample - loss: 4.8359e-05 - val_loss: 0.0012\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 292us/sample - loss: 4.7231e-05 - val_loss: 0.0019\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 304us/sample - loss: 4.2517e-05 - val_loss: 3.5686e-04\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 301us/sample - loss: 4.2257e-05 - val_loss: 3.8645e-04\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 307us/sample - loss: 4.3032e-05 - val_loss: 0.0015\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 305us/sample - loss: 4.0072e-05 - val_loss: 4.5122e-04\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 324us/sample - loss: 3.9108e-05 - val_loss: 3.9729e-04\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 314us/sample - loss: 3.8090e-05 - val_loss: 0.0012\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 320us/sample - loss: 3.7137e-05 - val_loss: 4.7570e-04\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 317us/sample - loss: 3.7674e-05 - val_loss: 6.1575e-04\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 326us/sample - loss: 3.5132e-05 - val_loss: 3.3286e-04\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 328us/sample - loss: 4.0627e-05 - val_loss: 3.1421e-04\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 356us/sample - loss: 3.6490e-05 - val_loss: 3.1072e-04\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 3.4963e-05 - val_loss: 3.6988e-04\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 401us/sample - loss: 3.3699e-05 - val_loss: 8.9934e-04\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 400us/sample - loss: 3.8171e-05 - val_loss: 0.0010\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 3.2821e-05 - val_loss: 3.3242e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 380us/sample - loss: 3.5341e-05 - val_loss: 9.2982e-04\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 398us/sample - loss: 3.5402e-05 - val_loss: 0.0016\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 406us/sample - loss: 3.4009e-05 - val_loss: 0.0020\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 377us/sample - loss: 3.5106e-05 - val_loss: 3.0802e-04\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 399us/sample - loss: 3.1350e-05 - val_loss: 3.0250e-04\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 374us/sample - loss: 3.5628e-05 - val_loss: 3.5462e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 371us/sample - loss: 3.2512e-05 - val_loss: 7.6565e-04\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 408us/sample - loss: 3.2103e-05 - val_loss: 0.0012\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 354us/sample - loss: 2.8057e-05 - val_loss: 7.4251e-04\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 371us/sample - loss: 3.1819e-05 - val_loss: 8.4976e-04\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 348us/sample - loss: 2.8972e-05 - val_loss: 5.5978e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 346us/sample - loss: 3.0963e-05 - val_loss: 4.9550e-04\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 346us/sample - loss: 2.9165e-05 - val_loss: 7.7192e-04\n",
      "rmse score :  [3785.76103616]\n",
      "mae score :  3192.607177734375\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 6s 2ms/sample - loss: 5.4507e-04 - val_loss: 0.0052\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 2s 717us/sample - loss: 1.5128e-04 - val_loss: 0.0034\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 482us/sample - loss: 1.1093e-04 - val_loss: 0.0017\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 2s 621us/sample - loss: 1.0414e-04 - val_loss: 0.0044\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 2s 548us/sample - loss: 9.6860e-05 - val_loss: 9.0472e-04\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 384us/sample - loss: 9.3960e-05 - val_loss: 6.5922e-04\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 364us/sample - loss: 9.0799e-05 - val_loss: 9.6230e-04\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 427us/sample - loss: 8.7275e-05 - val_loss: 8.4995e-04\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 6.9337e-05 - val_loss: 0.0011\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 459us/sample - loss: 7.6329e-05 - val_loss: 6.7906e-04\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 7.1542e-05 - val_loss: 0.0013\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 400us/sample - loss: 6.9437e-05 - val_loss: 8.2880e-04\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 381us/sample - loss: 6.9775e-05 - val_loss: 0.0025\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 459us/sample - loss: 6.0351e-05 - val_loss: 6.2018e-04\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 2s 544us/sample - loss: 6.1657e-05 - val_loss: 0.0011\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 488us/sample - loss: 6.2396e-05 - val_loss: 0.0014\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 487us/sample - loss: 5.5493e-05 - val_loss: 0.0037\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 477us/sample - loss: 6.1388e-05 - val_loss: 0.0021\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 492us/sample - loss: 5.3095e-05 - val_loss: 0.0014\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 500us/sample - loss: 5.7363e-05 - val_loss: 0.0032\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 502us/sample - loss: 5.4415e-05 - val_loss: 5.6514e-04\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2897/2897 [==============================] - 1s 456us/sample - loss: 4.8173e-05 - val_loss: 0.0011\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 447us/sample - loss: 5.0077e-05 - val_loss: 0.0011\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 418us/sample - loss: 4.8032e-05 - val_loss: 0.0010\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 514us/sample - loss: 4.8922e-05 - val_loss: 5.2304e-04\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 482us/sample - loss: 4.3911e-05 - val_loss: 0.0016\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 472us/sample - loss: 4.4601e-05 - val_loss: 7.2293e-04\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 426us/sample - loss: 4.5199e-05 - val_loss: 4.9008e-04\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 418us/sample - loss: 4.7317e-05 - val_loss: 0.0032\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 504us/sample - loss: 5.0276e-05 - val_loss: 4.6230e-04\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 487us/sample - loss: 4.0669e-05 - val_loss: 9.5803e-04\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 469us/sample - loss: 4.4587e-05 - val_loss: 6.9850e-04\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 427us/sample - loss: 4.3334e-05 - val_loss: 6.5428e-04\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 438us/sample - loss: 4.5689e-05 - val_loss: 0.0012\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 475us/sample - loss: 3.6973e-05 - val_loss: 7.9162e-04\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 492us/sample - loss: 4.3363e-05 - val_loss: 6.0571e-04\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 496us/sample - loss: 4.3397e-05 - val_loss: 5.7597e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 4.1738e-05 - val_loss: 6.8825e-04\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 4.1147e-05 - val_loss: 4.6454e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 428us/sample - loss: 3.3498e-05 - val_loss: 4.6403e-04\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 3.9279e-05 - val_loss: 0.0035\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 403us/sample - loss: 4.1085e-05 - val_loss: 6.4032e-04\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 408us/sample - loss: 3.6113e-05 - val_loss: 4.4738e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 402us/sample - loss: 3.9990e-05 - val_loss: 3.5764e-04\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 3.3057e-05 - val_loss: 9.8741e-04\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 487us/sample - loss: 3.3066e-05 - val_loss: 3.3560e-04\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 494us/sample - loss: 3.5752e-05 - val_loss: 0.0016\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 394us/sample - loss: 3.6198e-05 - val_loss: 2.9396e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 447us/sample - loss: 3.4244e-05 - val_loss: 3.7495e-04\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 493us/sample - loss: 3.4782e-05 - val_loss: 3.3959e-04\n",
      "rmse score :  [3025.82834106]\n",
      "mae score :  2307.78955078125\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 6s 2ms/sample - loss: 6.6891e-04 - val_loss: 0.0103\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 2s 593us/sample - loss: 1.3919e-04 - val_loss: 0.0028\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 427us/sample - loss: 1.1494e-04 - val_loss: 8.3333e-04\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 455us/sample - loss: 1.0593e-04 - val_loss: 0.0042\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 2s 608us/sample - loss: 9.5940e-05 - val_loss: 7.0613e-04\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 502us/sample - loss: 9.4398e-05 - val_loss: 0.0040\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 433us/sample - loss: 9.2822e-05 - val_loss: 0.0033\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 418us/sample - loss: 8.4819e-05 - val_loss: 6.6144e-04\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 432us/sample - loss: 8.3936e-05 - val_loss: 0.0052\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 2s 637us/sample - loss: 7.7941e-05 - val_loss: 0.0011\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 2s 527us/sample - loss: 7.3016e-05 - val_loss: 0.0014\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 2s 621us/sample - loss: 7.4823e-05 - val_loss: 7.1105e-04\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 2s 600us/sample - loss: 7.4268e-05 - val_loss: 0.0011\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 469us/sample - loss: 7.5601e-05 - val_loss: 0.0012\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 433us/sample - loss: 6.9215e-05 - val_loss: 0.0029\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 2s 541us/sample - loss: 7.2815e-05 - val_loss: 7.6335e-04\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 483us/sample - loss: 6.3633e-05 - val_loss: 0.0015\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 410us/sample - loss: 6.3933e-05 - val_loss: 0.0011\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 417us/sample - loss: 6.5688e-05 - val_loss: 6.9091e-04\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 507us/sample - loss: 6.0296e-05 - val_loss: 8.2654e-04\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 2s 527us/sample - loss: 6.1189e-05 - val_loss: 8.1342e-04\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 488us/sample - loss: 5.9878e-05 - val_loss: 6.7165e-04\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 472us/sample - loss: 5.9600e-05 - val_loss: 8.0660e-04\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 453us/sample - loss: 5.4461e-05 - val_loss: 6.8379e-04\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 5.7133e-05 - val_loss: 0.0027\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 433us/sample - loss: 5.9987e-05 - val_loss: 0.0018\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 481us/sample - loss: 5.7260e-05 - val_loss: 5.4129e-04\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 471us/sample - loss: 5.1791e-05 - val_loss: 6.4153e-04\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 450us/sample - loss: 5.1114e-05 - val_loss: 0.0016\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 428us/sample - loss: 5.2984e-05 - val_loss: 7.4936e-04\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 408us/sample - loss: 5.2777e-05 - val_loss: 8.9211e-04\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 5.2450e-05 - val_loss: 4.6637e-04\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 401us/sample - loss: 4.7550e-05 - val_loss: 0.0017\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 4.4427e-05 - val_loss: 7.7167e-04\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 410us/sample - loss: 4.5148e-05 - val_loss: 4.6808e-04\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 401us/sample - loss: 4.6176e-05 - val_loss: 0.0014\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 4.5546e-05 - val_loss: 4.7833e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 399us/sample - loss: 4.1582e-05 - val_loss: 0.0024\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 409us/sample - loss: 4.4016e-05 - val_loss: 9.0993e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 421us/sample - loss: 4.4842e-05 - val_loss: 0.0026\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 427us/sample - loss: 4.4514e-05 - val_loss: 6.4796e-04\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 514us/sample - loss: 4.6259e-05 - val_loss: 0.0025\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 473us/sample - loss: 3.8609e-05 - val_loss: 9.9574e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 471us/sample - loss: 4.2170e-05 - val_loss: 3.9992e-04\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 472us/sample - loss: 4.0550e-05 - val_loss: 8.8514e-04\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 466us/sample - loss: 3.8660e-05 - val_loss: 3.8314e-04\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 443us/sample - loss: 3.8088e-05 - val_loss: 0.0013\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 443us/sample - loss: 4.0973e-05 - val_loss: 9.5658e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 439us/sample - loss: 3.9086e-05 - val_loss: 6.7459e-04\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 451us/sample - loss: 3.8854e-05 - val_loss: 3.9424e-04\n",
      "rmse score :  [2720.39674373]\n",
      "mae score :  2115.791015625\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 5s 2ms/sample - loss: 6.1420e-04 - val_loss: 0.0088\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 1.7563e-04 - val_loss: 0.0043\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 492us/sample - loss: 1.4094e-04 - val_loss: 0.0023\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 441us/sample - loss: 1.4808e-04 - val_loss: 5.2842e-04\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 423us/sample - loss: 1.3030e-04 - val_loss: 9.8746e-04\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 419us/sample - loss: 1.1690e-04 - val_loss: 0.0010\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 402us/sample - loss: 1.0519e-04 - val_loss: 7.1088e-04\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 404us/sample - loss: 1.0244e-04 - val_loss: 0.0016\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 1.0183e-04 - val_loss: 6.2931e-04\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 395us/sample - loss: 9.2329e-05 - val_loss: 5.5912e-04\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 398us/sample - loss: 9.7588e-05 - val_loss: 0.0012\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 432us/sample - loss: 8.3498e-05 - val_loss: 0.0016\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 436us/sample - loss: 7.7972e-05 - val_loss: 8.1027e-04\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 7.4205e-05 - val_loss: 5.6113e-04\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 7.7548e-05 - val_loss: 0.0027\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 411us/sample - loss: 8.4177e-05 - val_loss: 0.0017\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 7.1922e-05 - val_loss: 8.8316e-04\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 393us/sample - loss: 7.4265e-05 - val_loss: 0.0021\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 393us/sample - loss: 6.6567e-05 - val_loss: 5.9580e-04\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 387us/sample - loss: 7.1937e-05 - val_loss: 0.0043\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 392us/sample - loss: 7.3528e-05 - val_loss: 6.3951e-04\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 394us/sample - loss: 6.2936e-05 - val_loss: 0.0012\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 441us/sample - loss: 6.4162e-05 - val_loss: 0.0020\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 486us/sample - loss: 6.3635e-05 - val_loss: 8.8814e-04\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 488us/sample - loss: 5.7915e-05 - val_loss: 7.6853e-04\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 482us/sample - loss: 6.4341e-05 - val_loss: 0.0017\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 461us/sample - loss: 6.0143e-05 - val_loss: 6.3091e-04\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 419us/sample - loss: 5.7054e-05 - val_loss: 0.0029\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 493us/sample - loss: 5.8062e-05 - val_loss: 0.0042\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 2s 667us/sample - loss: 5.5030e-05 - val_loss: 0.0010\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 5.8348e-05 - val_loss: 0.0019\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 411us/sample - loss: 5.8943e-05 - val_loss: 8.6704e-04\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 454us/sample - loss: 4.9563e-05 - val_loss: 8.3295e-04\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 388us/sample - loss: 5.8795e-05 - val_loss: 7.0911e-04\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 403us/sample - loss: 5.3404e-05 - val_loss: 9.5790e-04\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 379us/sample - loss: 4.9790e-05 - val_loss: 0.0014\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 381us/sample - loss: 5.3804e-05 - val_loss: 9.1769e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 374us/sample - loss: 5.1500e-05 - val_loss: 5.6476e-04\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 375us/sample - loss: 5.1033e-05 - val_loss: 7.1901e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 373us/sample - loss: 4.5912e-05 - val_loss: 5.8615e-04\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 376us/sample - loss: 4.8712e-05 - val_loss: 0.0017\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 372us/sample - loss: 4.4631e-05 - val_loss: 0.0044\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 370us/sample - loss: 4.5887e-05 - val_loss: 0.0019\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 364us/sample - loss: 4.6675e-05 - val_loss: 0.0011\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 361us/sample - loss: 4.6979e-05 - val_loss: 0.0011\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 386us/sample - loss: 4.4325e-05 - val_loss: 5.3053e-04\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 4.2748e-05 - val_loss: 6.7081e-04\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 380us/sample - loss: 4.4784e-05 - val_loss: 6.6035e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 4.2403e-05 - val_loss: 5.3788e-04\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 454us/sample - loss: 4.4675e-05 - val_loss: 0.0010\n",
      "rmse score :  [3256.82292889]\n",
      "mae score :  2669.355712890625\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 4s 1ms/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 366us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 361us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 358us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 367us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 368us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 396us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 508us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 469us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 432us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 421us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 418us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 460us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2897/2897 [==============================] - 1s 434us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 451us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 456us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 385us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 366us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 358us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 358us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 374us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 363us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 364us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 356us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 357us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 371us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 352us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 355us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 352us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 352us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 355us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 352us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 354us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 356us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 362us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 368us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 354us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 342us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 359us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 351us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 350us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 377us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 426us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 462us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 465us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 457us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 488us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 449us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 413us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "rmse score :  [49932.1790464]\n",
      "mae score :  49264.28515625\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 8s 3ms/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 462us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 418us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 437us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 453us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 470us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 428us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 476us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 483us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 480us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 434us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 431us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 456us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 441us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 417us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 411us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 413us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 429us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 419us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 418us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 446us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 434us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 433us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 420us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 422us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 434us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 455us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 437us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 429us/sample - loss: 0.0040 - val_loss: 0.1367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 398us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 398us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 395us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 445us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 430us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 440us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 400us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 409us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 418us/sample - loss: 0.0040 - val_loss: 0.1367\n",
      "rmse score :  [49932.1790464]\n",
      "mae score :  49264.28515625\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 4s 2ms/sample - loss: 8.2515e-04 - val_loss: 0.0078\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 450us/sample - loss: 2.1712e-04 - val_loss: 0.0057\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 427us/sample - loss: 1.5887e-04 - val_loss: 7.4569e-04\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 434us/sample - loss: 1.3663e-04 - val_loss: 8.1238e-04\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 1.4590e-04 - val_loss: 0.0010\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 501us/sample - loss: 1.2292e-04 - val_loss: 0.0023\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 499us/sample - loss: 1.2355e-04 - val_loss: 0.0026\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 456us/sample - loss: 1.1043e-04 - val_loss: 0.0012\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 478us/sample - loss: 1.1912e-04 - val_loss: 0.0011\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 489us/sample - loss: 1.0174e-04 - val_loss: 0.0013\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 502us/sample - loss: 1.0557e-04 - val_loss: 7.7349e-04\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 484us/sample - loss: 1.0392e-04 - val_loss: 0.0079\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 498us/sample - loss: 9.8890e-05 - val_loss: 0.0019\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 502us/sample - loss: 9.9806e-05 - val_loss: 8.2023e-04\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 500us/sample - loss: 9.9062e-05 - val_loss: 0.0013\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 480us/sample - loss: 8.7060e-05 - val_loss: 0.0017\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 449us/sample - loss: 8.2964e-05 - val_loss: 7.1096e-04\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 449us/sample - loss: 7.3485e-05 - val_loss: 0.0012\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 440us/sample - loss: 7.8813e-05 - val_loss: 0.0035\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 455us/sample - loss: 8.1503e-05 - val_loss: 0.0073\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 2s 534us/sample - loss: 8.2356e-05 - val_loss: 0.0015\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 421us/sample - loss: 7.3676e-05 - val_loss: 0.0011\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 436us/sample - loss: 7.2769e-05 - val_loss: 0.0016\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 422us/sample - loss: 7.0931e-05 - val_loss: 9.4963e-04\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 427us/sample - loss: 6.9530e-05 - val_loss: 0.0014\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 7.3531e-05 - val_loss: 0.0021\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 418us/sample - loss: 6.7962e-05 - val_loss: 7.4126e-04\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 436us/sample - loss: 6.7951e-05 - val_loss: 8.1930e-04\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 6.8968e-05 - val_loss: 0.0021\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 5.7944e-05 - val_loss: 0.0014\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 420us/sample - loss: 6.5863e-05 - val_loss: 0.0012\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 422us/sample - loss: 6.1429e-05 - val_loss: 0.0012\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 6.0734e-05 - val_loss: 0.0038\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 423us/sample - loss: 5.1747e-05 - val_loss: 0.0024\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 5.8199e-05 - val_loss: 9.2979e-04\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 436us/sample - loss: 5.9218e-05 - val_loss: 0.0011\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 417us/sample - loss: 5.9911e-05 - val_loss: 9.4098e-04\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 407us/sample - loss: 5.2822e-05 - val_loss: 0.0027\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 417us/sample - loss: 5.2093e-05 - val_loss: 8.3879e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 5.5529e-05 - val_loss: 0.0011\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 5.2600e-05 - val_loss: 8.6768e-04\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 5.4438e-05 - val_loss: 0.0010\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 5.4721e-05 - val_loss: 8.9852e-04\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 421us/sample - loss: 5.1828e-05 - val_loss: 0.0011\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 456us/sample - loss: 4.9129e-05 - val_loss: 0.0075\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 421us/sample - loss: 4.7905e-05 - val_loss: 0.0014\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 4.6227e-05 - val_loss: 0.0016\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 421us/sample - loss: 5.5521e-05 - val_loss: 0.0033\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 428us/sample - loss: 4.4731e-05 - val_loss: 0.0010\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 457us/sample - loss: 4.8444e-05 - val_loss: 0.0014\n",
      "rmse score :  [5970.37566048]\n",
      "mae score :  5125.68017578125\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 5s 2ms/sample - loss: 8.2493e-04 - val_loss: 0.0057\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 2.0959e-04 - val_loss: 0.0071\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 423us/sample - loss: 1.6955e-04 - val_loss: 0.0020\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 420us/sample - loss: 1.6996e-04 - val_loss: 0.0027\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 449us/sample - loss: 1.5316e-04 - val_loss: 0.0016\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 468us/sample - loss: 1.3449e-04 - val_loss: 0.0013\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 434us/sample - loss: 1.3340e-04 - val_loss: 6.6073e-04\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 440us/sample - loss: 1.1956e-04 - val_loss: 6.0452e-04\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 1.1429e-04 - val_loss: 0.0023\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 429us/sample - loss: 1.1238e-04 - val_loss: 0.0012\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 436us/sample - loss: 1.0904e-04 - val_loss: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 408us/sample - loss: 1.0309e-04 - val_loss: 0.0019\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 400us/sample - loss: 1.0700e-04 - val_loss: 9.5057e-04\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 411us/sample - loss: 9.7234e-05 - val_loss: 8.7159e-04\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 408us/sample - loss: 9.9124e-05 - val_loss: 7.7901e-04\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 400us/sample - loss: 1.0204e-04 - val_loss: 0.0010\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 410us/sample - loss: 9.4035e-05 - val_loss: 0.0010\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 400us/sample - loss: 9.1920e-05 - val_loss: 0.0028\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 438us/sample - loss: 9.3105e-05 - val_loss: 9.1792e-04\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 450us/sample - loss: 8.4095e-05 - val_loss: 0.0011\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 8.8689e-05 - val_loss: 0.0012\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 443us/sample - loss: 9.0500e-05 - val_loss: 0.0040\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 442us/sample - loss: 8.6092e-05 - val_loss: 0.0014\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 413us/sample - loss: 8.9708e-05 - val_loss: 0.0026\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 406us/sample - loss: 8.9475e-05 - val_loss: 0.0014\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 7.9293e-05 - val_loss: 0.0048\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 412us/sample - loss: 7.9007e-05 - val_loss: 0.0146\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 411us/sample - loss: 7.7286e-05 - val_loss: 0.0031\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 445us/sample - loss: 8.1145e-05 - val_loss: 0.0016\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 8.1659e-05 - val_loss: 0.0108\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 410us/sample - loss: 7.8570e-05 - val_loss: 0.0015\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 412us/sample - loss: 7.0747e-05 - val_loss: 0.0046\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 410us/sample - loss: 8.2191e-05 - val_loss: 0.0027\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 412us/sample - loss: 6.8425e-05 - val_loss: 0.0016\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 409us/sample - loss: 6.8858e-05 - val_loss: 0.0064\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 415us/sample - loss: 6.2798e-05 - val_loss: 0.0026\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 410us/sample - loss: 7.1174e-05 - val_loss: 0.0018\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 413us/sample - loss: 6.2925e-05 - val_loss: 0.0019\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 459us/sample - loss: 6.7910e-05 - val_loss: 9.5432e-04\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 422us/sample - loss: 6.4267e-05 - val_loss: 0.0010\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 417us/sample - loss: 6.5985e-05 - val_loss: 9.3071e-04\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 419us/sample - loss: 6.3616e-05 - val_loss: 0.0011\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 6.3738e-05 - val_loss: 0.0013\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 477us/sample - loss: 6.1331e-05 - val_loss: 0.0018\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 428us/sample - loss: 5.9655e-05 - val_loss: 8.4101e-04\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 420us/sample - loss: 5.9856e-05 - val_loss: 7.2085e-04\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 422us/sample - loss: 5.7936e-05 - val_loss: 6.9197e-04\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 416us/sample - loss: 5.4776e-05 - val_loss: 7.4378e-04\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 419us/sample - loss: 5.7470e-05 - val_loss: 7.9203e-04\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 413us/sample - loss: 5.5702e-05 - val_loss: 7.3893e-04\n",
      "rmse score :  [4357.93517943]\n",
      "mae score :  3507.6240234375\n",
      "Train on 2897 samples, validate on 611 samples\n",
      "Epoch 1/50\n",
      "2897/2897 [==============================] - 6s 2ms/sample - loss: 7.9563e-04 - val_loss: 0.0079\n",
      "Epoch 2/50\n",
      "2897/2897 [==============================] - 1s 426us/sample - loss: 2.1176e-04 - val_loss: 0.0014\n",
      "Epoch 3/50\n",
      "2897/2897 [==============================] - 1s 425us/sample - loss: 2.0406e-04 - val_loss: 5.5923e-04\n",
      "Epoch 4/50\n",
      "2897/2897 [==============================] - 1s 458us/sample - loss: 1.6214e-04 - val_loss: 0.0025\n",
      "Epoch 5/50\n",
      "2897/2897 [==============================] - 1s 419us/sample - loss: 1.4837e-04 - val_loss: 0.0085\n",
      "Epoch 6/50\n",
      "2897/2897 [==============================] - 1s 485us/sample - loss: 1.5661e-04 - val_loss: 0.0017\n",
      "Epoch 7/50\n",
      "2897/2897 [==============================] - 1s 489us/sample - loss: 1.3548e-04 - val_loss: 0.0035\n",
      "Epoch 8/50\n",
      "2897/2897 [==============================] - 1s 452us/sample - loss: 1.3577e-04 - val_loss: 7.3647e-04\n",
      "Epoch 9/50\n",
      "2897/2897 [==============================] - 1s 464us/sample - loss: 1.2523e-04 - val_loss: 7.7544e-04\n",
      "Epoch 10/50\n",
      "2897/2897 [==============================] - 1s 428us/sample - loss: 1.1989e-04 - val_loss: 0.0017\n",
      "Epoch 11/50\n",
      "2897/2897 [==============================] - 1s 451us/sample - loss: 1.1365e-04 - val_loss: 0.0012\n",
      "Epoch 12/50\n",
      "2897/2897 [==============================] - 1s 431us/sample - loss: 1.0426e-04 - val_loss: 0.0019\n",
      "Epoch 13/50\n",
      "2897/2897 [==============================] - 1s 441us/sample - loss: 1.0227e-04 - val_loss: 0.0022\n",
      "Epoch 14/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 1.1044e-04 - val_loss: 0.0157\n",
      "Epoch 15/50\n",
      "2897/2897 [==============================] - 1s 435us/sample - loss: 1.0826e-04 - val_loss: 0.0072\n",
      "Epoch 16/50\n",
      "2897/2897 [==============================] - 1s 431us/sample - loss: 1.0179e-04 - val_loss: 0.0034\n",
      "Epoch 17/50\n",
      "2897/2897 [==============================] - 1s 486us/sample - loss: 9.9073e-05 - val_loss: 9.9454e-04\n",
      "Epoch 18/50\n",
      "2897/2897 [==============================] - 1s 449us/sample - loss: 9.9007e-05 - val_loss: 8.4214e-04\n",
      "Epoch 19/50\n",
      "2897/2897 [==============================] - 1s 434us/sample - loss: 1.0058e-04 - val_loss: 0.0012\n",
      "Epoch 20/50\n",
      "2897/2897 [==============================] - 1s 440us/sample - loss: 8.6623e-05 - val_loss: 0.0011\n",
      "Epoch 21/50\n",
      "2897/2897 [==============================] - 1s 422us/sample - loss: 8.5542e-05 - val_loss: 0.0012\n",
      "Epoch 22/50\n",
      "2897/2897 [==============================] - 1s 419us/sample - loss: 7.7293e-05 - val_loss: 0.0010\n",
      "Epoch 23/50\n",
      "2897/2897 [==============================] - 1s 447us/sample - loss: 9.2328e-05 - val_loss: 9.2787e-04\n",
      "Epoch 24/50\n",
      "2897/2897 [==============================] - 1s 440us/sample - loss: 7.5499e-05 - val_loss: 0.0013\n",
      "Epoch 25/50\n",
      "2897/2897 [==============================] - 1s 432us/sample - loss: 8.9368e-05 - val_loss: 0.0014\n",
      "Epoch 26/50\n",
      "2897/2897 [==============================] - 1s 431us/sample - loss: 7.4519e-05 - val_loss: 0.0022\n",
      "Epoch 27/50\n",
      "2897/2897 [==============================] - 1s 420us/sample - loss: 7.7643e-05 - val_loss: 0.0011\n",
      "Epoch 28/50\n",
      "2897/2897 [==============================] - 1s 481us/sample - loss: 6.6630e-05 - val_loss: 0.0014\n",
      "Epoch 29/50\n",
      "2897/2897 [==============================] - 1s 450us/sample - loss: 6.6647e-05 - val_loss: 0.0024\n",
      "Epoch 30/50\n",
      "2897/2897 [==============================] - 1s 473us/sample - loss: 7.9119e-05 - val_loss: 0.0019\n",
      "Epoch 31/50\n",
      "2897/2897 [==============================] - 1s 436us/sample - loss: 6.5478e-05 - val_loss: 0.0022\n",
      "Epoch 32/50\n",
      "2897/2897 [==============================] - 1s 431us/sample - loss: 6.9192e-05 - val_loss: 0.0014\n",
      "Epoch 33/50\n",
      "2897/2897 [==============================] - 1s 439us/sample - loss: 6.9114e-05 - val_loss: 0.0015\n",
      "Epoch 34/50\n",
      "2897/2897 [==============================] - 1s 406us/sample - loss: 6.1871e-05 - val_loss: 0.0163\n",
      "Epoch 35/50\n",
      "2897/2897 [==============================] - 1s 399us/sample - loss: 6.1838e-05 - val_loss: 0.0039\n",
      "Epoch 36/50\n",
      "2897/2897 [==============================] - 1s 402us/sample - loss: 6.3478e-05 - val_loss: 0.0082\n",
      "Epoch 37/50\n",
      "2897/2897 [==============================] - 1s 438us/sample - loss: 6.4629e-05 - val_loss: 0.0025\n",
      "Epoch 38/50\n",
      "2897/2897 [==============================] - 1s 426us/sample - loss: 6.3034e-05 - val_loss: 0.0035\n",
      "Epoch 39/50\n",
      "2897/2897 [==============================] - 1s 403us/sample - loss: 5.8535e-05 - val_loss: 0.0153\n",
      "Epoch 40/50\n",
      "2897/2897 [==============================] - 1s 417us/sample - loss: 5.9119e-05 - val_loss: 0.0026\n",
      "Epoch 41/50\n",
      "2897/2897 [==============================] - 1s 424us/sample - loss: 5.8125e-05 - val_loss: 0.0024\n",
      "Epoch 42/50\n",
      "2897/2897 [==============================] - 1s 414us/sample - loss: 5.8447e-05 - val_loss: 0.0058\n",
      "Epoch 43/50\n",
      "2897/2897 [==============================] - 1s 427us/sample - loss: 5.3007e-05 - val_loss: 0.0058\n",
      "Epoch 44/50\n",
      "2897/2897 [==============================] - 1s 423us/sample - loss: 5.4928e-05 - val_loss: 0.0013\n",
      "Epoch 45/50\n",
      "2897/2897 [==============================] - 1s 428us/sample - loss: 5.6840e-05 - val_loss: 0.0195\n",
      "Epoch 46/50\n",
      "2897/2897 [==============================] - 1s 436us/sample - loss: 5.0895e-05 - val_loss: 0.0021\n",
      "Epoch 47/50\n",
      "2897/2897 [==============================] - 1s 450us/sample - loss: 5.7972e-05 - val_loss: 0.0037\n",
      "Epoch 48/50\n",
      "2897/2897 [==============================] - 1s 457us/sample - loss: 5.2482e-05 - val_loss: 0.0017\n",
      "Epoch 49/50\n",
      "2897/2897 [==============================] - 1s 446us/sample - loss: 4.7193e-05 - val_loss: 0.0062\n",
      "Epoch 50/50\n",
      "2897/2897 [==============================] - 1s 432us/sample - loss: 5.2070e-05 - val_loss: 0.0228\n",
      "rmse score :  [24548.02893063]\n",
      "mae score :  23415.3515625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'number_dense_layers')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def benchmark_number_dense_layers(n):\n",
    "    number_dense_layers = [i for i in range(0,n)]\n",
    "    rmse_score = []\n",
    "    \n",
    "    for i in range (0,n) :\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        model.add(keras.Input(shape=(3,1)))\n",
    "        layer_1 = layers.LSTM(128, activation =\"relu\", return_sequences=True)\n",
    "        model.add(layer_1)\n",
    "\n",
    "        #Dropout permet d'éviter l'overfitting en remplacant certains input par 0 pendant le set de training\n",
    "        layer_2 = layers.Dropout(0.2)\n",
    "        model.add(layer_2)\n",
    "\n",
    "        layer_3 = layers.GRU(128, activation =\"relu\")\n",
    "        model.add(layer_3)\n",
    "        \n",
    "        for j in range(0,i):\n",
    "            model.add(layers.Dense(32,activation ='relu'))\n",
    "\n",
    "        layer_5 = layers.Dense(1,activation ='relu', input_shape=(1,0))\n",
    "        model.add(layer_5)\n",
    "\n",
    "        model.compile(loss= mse,optimizer='rmsprop')\n",
    "\n",
    "        #Fitting the Recurrent Neural Network\n",
    "        model.fit(x = X_train, y = y_train, validation_data = (X_val, y_val),batch_size = 32, epochs = 50)\n",
    "        \n",
    "        rmse_score.append(evaluation(model,X_test)[0][0])\n",
    "    \n",
    "    return number_dense_layers, rmse_score\n",
    "\n",
    "number_dense_layers, rmse_score = benchmark_number_dense_layers(9)\n",
    "plt.plot(number_dense_layers, rmse_score)\n",
    "plt.ylabel(\"rmse score\")\n",
    "plt.xlabel(\"number_dense_layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'number_dense_layers')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(number_dense_layers, rmse_score)\n",
    "plt.ylabel(\"rmse score\")\n",
    "plt.xlabel(\"number_dense_layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2720.396743728205"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(rmse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultats pour un test de taille 30**\n",
    "- 1 LSTM(128), 4 Dense(64,32,32,1) et activations linéaires : rmse 3136.8 et 2470.7\n",
    "- 1 LSTM(128), 1 Dense(1) et activations linéaires : rmse 2998.2 et 2490.6\n",
    "\n",
    "\n",
    "- 1 LSTM(128), 1 Dropout(0.1), 1 GRU(128), 2 Dense(32,1); activations relu, optimizer='rmsprop', rmse = 2734.18, mae = 2229\n",
    "- 1 LSTM(128), 1 GRU(128), 2 Dense(32,1); activations relu, optimizer='rmsprop', rmse = 5271.1, mae = 4587.2\n",
    "=> sans couche drop out les résultats sont bien moins bon, on essaye donc d'en rajouter une autre couche\n",
    "\n",
    "- 1 LSTM(128), 1 Dropout(0.1), 1 GRU(128), 1 Dropout(0.1), 2 Dense(32,1); activations relu, optimizer='rmsprop', rmse = 3368.5, mae = 2653.1 ; 2ème essai : rmse = 5628.5 , mae = 5069.9\n",
    "\n",
    "=> on observe une grande variabilité dans les résultats pour la même architecture du réseau de neurone\n",
    "\n",
    "=> les résultats sont moins bon, je retire la 2ème couche de dropout mais j'augmente le coefficient de la premiere couche de dropout: \n",
    "- 1 LSTM(128), 1 Dropout(0.3), 1 GRU(128), 2 Dense(32,1); activations relu, optimizer='rmsprop', rmse = 2734.18, mae = 2229\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LA TAILLE DU TEST ÉTAIT DE 350 ET PAS DE 30**\n",
    "\n",
    "Résultats :\n",
    "batch_size : 32, epochs : 50, optimizer='rmsprop'\n",
    "<br/> Variation du **type de fonction d'activation** :\n",
    "- 1 LSTM(128), 4 Dense(64,32,32,1) et activations linéaires : rmse 1329.4\n",
    "- 1 LSTM(128), 4 Dense(64,32,32,1) et activations relu : rmse 2999.8\n",
    "- 1 LSTM(128), 4 Dense(64,32,32,1) et activations sigmoid : rmse 15441.1\n",
    "- 1 LSTM(128), 4 Dense(64,32,32,1) et activations tanh : rmse 12519.1\n",
    "- 1 LSTM(128), 4 Dense(64,32,32,1) et 3 activations linéaires et tanh sur le dernier : rmse 4983.8\n",
    "=> Les fonctions d'activations linéaires sur les couches denses permettent d'avoir plus de résultats\n",
    "\n",
    "**Variation du nombre de unit dans les couches de réseaux de neurones** :\n",
    "- 1 LSTM(256), 4 Dense(128,128,64,1) et activations linéaires : rmse 1448.3\n",
    "- 1 LSTM(512), 4 Dense(512,256,128,1) et activations linéaires : rmse 27381.9\n",
    "<br/> => On obtient des moins bon résultats en augmentant le nombre d'unit, le résultat est particulièrement mauvais dans le deuxième cas, il s'agit d'overfitting\n",
    "Cette erreur venait du fait que le batch_size était resté à 32, en choissisant un batch_size = 512 : (toujours fonction d'activation linéaire):\n",
    "- 1 LSTM(512), 4 Dense(512,256,128,1) et activations linéaires : rmse 1803.2\n",
    "ce résultat n'est cependant toujours pas notre meilleur résultat\n",
    "\n",
    "**Différente fonctions de perte** :\n",
    "Je garde le meilleur modèle trouvé jusqu'à présent et je teste différentes fonctions de perte : <br/> 1 LSTM(128), 4 Dense(64,32,32,1) et activations linéaires\n",
    "- RMSE (Root Mean Squared Error) : rmse_score : 1631.7 ; mae_score : 911.9\n",
    "- MAE (Mean Absolute Error): rmse_score : 3935.7  ; mae_score : 2030.9\n",
    "- MAE (Mean Absolute Error) et fonction d'activation relu: rmse_score : 6761.8  ; mae_score : 4018.4\n",
    "- Huber Loss (moins sensible au données aberrante que le rmse): rmse_score : 3316.2  ; mae_score : 2098.3\n",
    "- Mean Squared Logarithmic Error : rmse_score : 30499.6 , mae_score : 23245.7\n",
    "- Cosine_similarity function : rmse_score : 473273.5 , mae_score : 472767.7\n",
    "\n",
    "<br/> Le meilleur résultat en termes de RMSE et de MAE est obtenu avec la fonction de perte RMSE. Un résultat plutôt bon est obtenu également avec la fonction Huber Loss qui utilise aussi des différences quadratiques mais est moins sensible au données aberrantes. Ici la sensibilité au données aberrantes est particulièrement importante car les données de tests concernent les dernières données où la volatilité est élevée.\n",
    "\n",
    "**Différents type d'optimizers**:\n",
    "Jusqu'ici, j'ai utilisé optimizer='rmsprop'. Je teste d'autre optimizer en utilisant les meilleurs paramètres trouvés, c'est à dire :\n",
    "<br/> LSTM(128), 4 Dense(64,32,32,1), activation linéaire, fonction de perte RSE\n",
    "- fonction d'optimisation adam : rmse_score : 1416.3 ; mae_score : 817.5\n",
    "- fonction d'optimisation rmsprop : rmse_score : 2033.0 ; mae_score : 1266.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "case_study_rnn_group_n",
   "notebookOrigID": 4256609446685350,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "mds-env",
   "language": "python",
   "name": "mds-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
